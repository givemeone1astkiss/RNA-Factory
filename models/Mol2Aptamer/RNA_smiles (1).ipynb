{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d782d5e-b56b-4c9f-bdc2-dbe51f2a5fc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.12/site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /root/miniconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95b7b1f8-bef9-402a-ad9e-8b3e9efc0b5d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: openpyxl in /root/miniconda3/lib/python3.12/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /root/miniconda3/lib/python3.12/site-packages (from openpyxl) (2.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05af7cb9-437f-48b3-9aaf-935559ce1024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def preprocess(input_csv, output_json):\n",
    "    # 读取Excel文件\n",
    "    df = pd.read_excel(input_csv)\n",
    "    \n",
    "    # 按 SMILES 分组\n",
    "    grouped = defaultdict(lambda: {\"sequences\": [], \"features\": {}})    \n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        smi = row[\"SMILES\"]\n",
    "        # 获取序列并将DNA转为RNA（T→U）\n",
    "        dna_seq = row[\"Sequence\"]\n",
    "        rna_seq = dna_seq.replace('T', 'U')  # 将DNA序列中的T替换为U，转换为RNA\n",
    "        \n",
    "        # 提取全局分子特征\n",
    "        features = {            \n",
    "            \"molecular_weight\": row.get(\"molecular_weight\", None),            \n",
    "            \"pubchem_id\": row.get(\"Pubchem ID\", None),            \n",
    "            \"title\": row.get(\"Titles\", None),            \n",
    "            \"iupac\": row.get(\"IUPAC Names\", None),            \n",
    "            \"condition\": row.get(\"Binding Conditions/Buffer\", None),        \n",
    "        }\n",
    "        \n",
    "        # 如果是该SMILES的第一条记录，设置特征\n",
    "        if not grouped[smi][\"features\"]:\n",
    "            grouped[smi][\"features\"] = features\n",
    "        \n",
    "        # 添加转换后的RNA序列\n",
    "        grouped[smi][\"sequences\"].append(rna_seq)\n",
    "    \n",
    "    # 转存为list格式\n",
    "    dataset = []    \n",
    "    for smi, v in grouped.items():\n",
    "        dataset.append({            \n",
    "            \"smiles\": smi,            \n",
    "            \"features\": v[\"features\"],            \n",
    "            \"sequences\": v[\"sequences\"]        \n",
    "        })\n",
    "    \n",
    "    # 保存为JSON文件\n",
    "    with open(output_json, \"w\") as f:\n",
    "        json.dump(dataset, f, indent=2)\n",
    "\n",
    "# 用法示例：\n",
    "preprocess(\"merged_file_with_smiles_filled.xlsx\", \"dataset.json\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce63b8b2-3a05-48da-94b4-4c1af8722e74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: torch in /root/miniconda3/lib/python3.12/site-packages (2.3.0+cu121)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.12/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /root/miniconda3/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/lib/python3.12/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /root/miniconda3/lib/python3.12/site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/miniconda3/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/miniconda3/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/miniconda3/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/miniconda3/lib/python3.12/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/miniconda3/lib/python3.12/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/miniconda3/lib/python3.12/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/miniconda3/lib/python3.12/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/miniconda3/lib/python3.12/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/miniconda3/lib/python3.12/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /root/miniconda3/lib/python3.12/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/miniconda3/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/miniconda3/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /root/miniconda3/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1f1bb76-379e-4ffd-a4f3-2ffda66530ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "\n",
    "\"\"\"\n",
    "字符级 tokenization\n",
    "\"\"\"\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = [\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"] + tokens\n",
    "        self.stoi = {s:i for i,s in enumerate(self.tokens)}\n",
    "        self.itos = {i:s for s,i in self.stoi.items()}\n",
    "\n",
    "    def encode(self, s, add_bos=True, add_eos=True):\n",
    "        tokens = list(s)\n",
    "        ids = []\n",
    "        if add_bos: ids.append(self.stoi[\"<bos>\"])\n",
    "        ids += [self.stoi.get(t, self.stoi[\"<unk>\"]) for t in tokens]\n",
    "        if add_eos: ids.append(self.stoi[\"<eos>\"])\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return \"\".join([self.itos[i] for i in ids if i not in (0,1,2)])\n",
    "\n",
    "    @property\n",
    "    def pad_id(self): return self.stoi[\"<pad>\"]\n",
    "\n",
    "class AptamerDataset(Dataset):\n",
    "    def __init__(self, json_path, smiles_vocab, seq_vocab, max_len=100):\n",
    "        with open(json_path) as f:\n",
    "            self.data = json.load(f)\n",
    "        self.smiles_vocab = smiles_vocab\n",
    "        self.seq_vocab = seq_vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        smiles = entry[\"smiles\"]\n",
    "        seqs = entry[\"sequences\"]  # 多个 aptamer\n",
    "\n",
    "        # Encode SMILES\n",
    "        smiles_ids = self.smiles_vocab.encode(smiles, add_bos=False, add_eos=False)\n",
    "\n",
    "        # Encode 所有 aptamer 序列\n",
    "        seq_ids = [self.seq_vocab.encode(s) for s in seqs]\n",
    "\n",
    "        return {\n",
    "            \"smiles_ids\": torch.tensor(smiles_ids, dtype=torch.long),\n",
    "            \"sequences\": [torch.tensor(s, dtype=torch.long) for s in seq_ids]\n",
    "        }\n",
    "\n",
    "def collate_fn(batch, pad_id=0):\n",
    "    # Pad SMILES\n",
    "    smiles_lens = [len(b[\"smiles_ids\"]) for b in batch]\n",
    "    max_smi = max(smiles_lens)\n",
    "    smiles_tensor = torch.full((len(batch), max_smi), pad_id, dtype=torch.long)\n",
    "    for i,b in enumerate(batch):\n",
    "        smiles_tensor[i,:len(b[\"smiles_ids\"])] = b[\"smiles_ids\"]\n",
    "\n",
    "    # 这里 sequences 是 list[list[tensor]]，训练时要 special handle\n",
    "    seqs_list = [b[\"sequences\"] for b in batch]\n",
    "\n",
    "    return {\n",
    "        \"smiles\": smiles_tensor,\n",
    "        \"sequences\": seqs_list\n",
    "    }\n",
    "\n",
    "# 用法\n",
    "# smiles_vocab = Vocab(list(\"CNOPSH[]=()#123456789\"))  # 自定义\n",
    "# seq_vocab = Vocab(list(\"ACGU\"))\n",
    "# dataset = AptamerDataset(\"dataset.json\", smiles_vocab, seq_vocab)\n",
    "# loader = DataLoader(dataset, batch_size=4, collate_fn=lambda x: collate_fn(x, pad_id=smiles_vocab.pad_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4f53c86-ef71-4155-8af9-a0cbf2ba9ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_vocab = Vocab(list(\"CNOPSH[]=()#123456789\")) \n",
    "seq_vocab = Vocab(list(\"ACGU\"))\n",
    "dataset = AptamerDataset(\"dataset.json\", smiles_vocab, seq_vocab)\n",
    "loader = DataLoader(dataset, batch_size=4, collate_fn=lambda x: collate_fn(x, pad_id=smiles_vocab.pad_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd2a97c0-e3d3-47e4-8861-bac4e3aa69d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\"\"\"\n",
    "模型骨架（Encoder-Decoder + MIL loss）\n",
    "\"\"\"\n",
    "\n",
    "class SimpleMolEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead=8, dim_feedforward=512),\n",
    "            num_layers=4\n",
    "        )\n",
    "\n",
    "    def forward(self, smiles_ids):\n",
    "        x = self.embed(smiles_ids).transpose(0,1)  # (L,B,D)\n",
    "        x = self.encoder(x)  # (L,B,D)\n",
    "        mol_repr = x.mean(0) # (B,D)\n",
    "        return mol_repr\n",
    "\n",
    "class RNASeqDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model, nhead=8, dim_feedforward=512),\n",
    "            num_layers=4\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, tgt_ids, memory):\n",
    "        tgt_emb = self.embed(tgt_ids).transpose(0,1)  # (L,B,D)\n",
    "        out = self.decoder(tgt_emb, memory.unsqueeze(0)) # memory (1,B,D)\n",
    "        logits = self.fc_out(out).transpose(0,1)  # (B,L,V)\n",
    "        return logits\n",
    "\n",
    "class Mol2Aptamer(nn.Module):\n",
    "    def __init__(self, smi_vocab, seq_vocab, d_model=256):\n",
    "        super().__init__()\n",
    "        self.encoder = SimpleMolEncoder(len(smi_vocab.tokens), d_model)\n",
    "        self.decoder = RNASeqDecoder(len(seq_vocab.tokens), d_model)\n",
    "\n",
    "    def forward(self, smiles_ids, seq_ids):\n",
    "        memory = self.encoder(smiles_ids) # (B,D)\n",
    "        logits = self.decoder(seq_ids, memory)\n",
    "        return logits\n",
    "\n",
    "def mil_loss(logits_list, seqs_list, pad_id):\n",
    "    \"\"\"\n",
    "    logits_list: list of [B,L,V] from decoder for each sequence candidate\n",
    "    seqs_list:  list of list[tensor], each tensor is (L,)\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    for b, seqs in enumerate(seqs_list):\n",
    "        logps = []\n",
    "        for seq in seqs:\n",
    "            tgt = seq[1:]  # skip BOS\n",
    "            inp = seq[:-1]\n",
    "            logit = logits_list[b][:len(inp)]\n",
    "            logp = F.cross_entropy(\n",
    "                logit, tgt, ignore_index=pad_id, reduction=\"sum\"\n",
    "            )\n",
    "            logps.append(-logp)\n",
    "        losses.append(-torch.logsumexp(torch.tensor(logps), dim=0))\n",
    "    return torch.stack(losses).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d7f94db-f5ad-4bcb-8a0f-10eb52a4e056",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: tokenizers in /root/miniconda3/lib/python3.12/site-packages (0.21.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /root/miniconda3/lib/python3.12/site-packages (from tokenizers) (0.34.4)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (1.1.8)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /root/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0542bdca-039d-4449-a3a1-a05682382268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "\"\"\"\n",
    "BPE tokenizer\n",
    "\"\"\"\n",
    "\n",
    "def train_bpe_tokenizer(corpus_file, vocab_size=2000, save_path=\"tokenizer.json\"):\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "    trainer = BpeTrainer(vocab_size=vocab_size, special_tokens=[\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"])\n",
    "    tokenizer.train([corpus_file], trainer)\n",
    "    tokenizer.post_processor = processors.TemplateProcessing(\n",
    "        single=\"<bos> $A <eos>\",\n",
    "        special_tokens=[(\"<bos>\",1), (\"<eos>\",2)]\n",
    "    )\n",
    "    tokenizer.save(save_path)\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0aac2cdd-a352-4d80-bc04-2705e9d3834c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tokenizer(version=\"1.0\", truncation=None, padding=None, added_tokens=[{\"id\":0, \"content\":\"<pad>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":1, \"content\":\"<bos>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":2, \"content\":\"<eos>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":3, \"content\":\"<unk>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}], normalizer=None, pre_tokenizer=ByteLevel(add_prefix_space=True, trim_offsets=True, use_regex=True), post_processor=TemplateProcessing(single=[SpecialToken(id=\"<bos>\", type_id=0), Sequence(id=A, type_id=0), SpecialToken(id=\"<eos>\", type_id=0)], pair=[Sequence(id=A, type_id=0), Sequence(id=B, type_id=1)], special_tokens={\"<bos>\":SpecialToken(id=\"<bos>\", ids=[1], tokens=[\"<bos>\"]), \"<eos>\":SpecialToken(id=\"<eos>\", ids=[2], tokens=[\"<eos>\"])}), decoder=None, model=BPE(dropout=None, unk_token=None, continuing_subword_prefix=None, end_of_word_suffix=None, fuse_unk=False, byte_fallback=False, ignore_merges=False, vocab={\"<pad>\":0, \"<bos>\":1, \"<eos>\":2, \"<unk>\":3, \"\"\":4, \"A\":5, \"C\":6, \"G\":7, \"N\":8, \"R\":9, \"U\":10, \"X\":11, \"Y\":12, \"t\":13, \"Ċ\":14, \"Ġ\":15, \"GG\":16, \"AU\":17, \"CC\":18, \"GU\":19, \"CU\":20, \"AA\":21, \"GA\":22, \"GC\":23, \"CA\":24, \"UU\":25, \"GGU\":26, \"GCU\":27, \"GAU\":28, \"GCC\":29, \"GAA\":30, \"AUU\":31, \"AC\":32, \"AGG\":33, \"GUU\":34, \"ACC\":35, \"CGU\":36, \"CGG\":37, \"GCA\":38, \"CUU\":39, \"AGU\":40, \"CCU\":41, \"ĠGG\":42, \"CGA\":43, \"GGGU\":44, \"AAU\":45, \"CAU\":46, \"CAA\":47, \"GCUU\":48, \"GCGU\":49, \"GCGG\":50, \"GAGG\":51, \"ACU\":52, \"GUGU\":53, \"AUCC\":54, \"UUU\":55, \"ACA\":56, \"AUGG\":57, \"AAAA\":58, \"GACU\":59, \"AGCC\":60, \"AGA\":61, \"AGC\":62, \"AUCU\":63, \"CGC\":64, \"GAUU\":65, \"CGGU\":66, \"CGCC\":67, \"CUGG\":68, \"CACA\":69, \"CGAA\":70, \"GAAU\":71, \"GUUU\":72, \"ĠU\":73, \"AAGG\":74, \"GAGA\":75, \"CACC\":76, \"GGCU\":77, \"GACC\":78, \"GUCU\":79, \"GUCC\":80, \"GCGC\":81, \"GGGG\":82, \"AGCUU\":83, \"GUGG\":84, \"ACGGU\":85, \"CGAU\":86, \"CACU\":87, \"AGGU\":88, \"CGCU\":89, \"AUUCA\":90, \"AGAU\":91, \"AACC\":92, \"ACCU\":93, \"CAUU\":94, \"ĠAU\":95, \"GUAA\":96, \"GGAU\":97, \"GGUU\":98, ...}, merges=[(\"G\", \"G\"), (\"A\", \"U\"), (\"C\", \"C\"), (\"G\", \"U\"), (\"C\", \"U\"), (\"A\", \"A\"), (\"G\", \"A\"), (\"G\", \"C\"), (\"C\", \"A\"), (\"U\", \"U\"), (\"GG\", \"U\"), (\"G\", \"CU\"), (\"G\", \"AU\"), (\"G\", \"CC\"), (\"G\", \"AA\"), (\"AU\", \"U\"), (\"A\", \"C\"), (\"A\", \"GG\"), (\"GU\", \"U\"), (\"A\", \"CC\"), (\"C\", \"GU\"), (\"C\", \"GG\"), (\"GC\", \"A\"), (\"CU\", \"U\"), (\"A\", \"GU\"), (\"CC\", \"U\"), (\"Ġ\", \"GG\"), (\"C\", \"GA\"), (\"GG\", \"GU\"), (\"A\", \"AU\"), (\"C\", \"AU\"), (\"C\", \"AA\"), (\"GCU\", \"U\"), (\"GC\", \"GU\"), (\"GC\", \"GG\"), (\"GA\", \"GG\"), (\"A\", \"CU\"), (\"GU\", \"GU\"), (\"AU\", \"CC\"), (\"UU\", \"U\"), (\"A\", \"CA\"), (\"AU\", \"GG\"), (\"AA\", \"AA\"), (\"GA\", \"CU\"), (\"A\", \"GCC\"), (\"A\", \"GA\"), (\"A\", \"GC\"), (\"AU\", \"CU\"), (\"C\", \"GC\"), (\"GAU\", \"U\"), (\"C\", \"GGU\"), (\"C\", \"GCC\"), (\"CU\", \"GG\"), (\"CA\", \"CA\"), (\"C\", \"GAA\"), (\"GA\", \"AU\"), (\"GU\", \"UU\"), (\"Ġ\", \"U\"), (\"AA\", \"GG\"), (\"GA\", \"GA\"), (\"CA\", \"CC\"), (\"GG\", \"CU\"), (\"GA\", \"CC\"), (\"GU\", \"CU\"), (\"GU\", \"CC\"), (\"GC\", \"GC\"), (\"GG\", \"GG\"), (\"A\", \"GCUU\"), (\"GU\", \"GG\"), (\"AC\", \"GGU\"), (\"C\", \"GAU\"), (\"CA\", \"CU\"), (\"A\", \"GGU\"), (\"C\", \"GCU\"), (\"AUU\", \"CA\"), (\"A\", \"GAU\"), (\"AA\", \"CC\"), (\"ACC\", \"U\"), (\"C\", \"AUU\"), (\"Ġ\", \"AU\"), (\"GU\", \"AA\"), (\"GG\", \"AU\"), (\"GG\", \"UU\"), (\"GU\", \"AGG\")]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bpe_tokenizer(\"all_smiles.txt\", vocab_size=500, save_path=\"/root/autodl-tmp/smiles_tokenizer.json\")\n",
    "train_bpe_tokenizer(\"all_rna_converted.txt\", vocab_size=100, save_path=\"/root/autodl-tmp/rna_tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e77e4e1-f8ab-4e39-8d45-a4092395392c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换完成！\n",
      "输入文件：all_rna.txt\n",
      "输出文件：all_rna_converted.txt\n",
      "总计转换序列数量：796\n"
     ]
    }
   ],
   "source": [
    "def dna_to_rna(input_file_path, output_file_path):\n",
    "    \"\"\"\n",
    "    将文本文件中的所有DNA序列（含T）转换为RNA序列（T→U）\n",
    "    :param input_file_path: 输入文本文件路径（如 all_rna.txt）\n",
    "    :param output_file_path: 输出RNA序列文件路径\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. 读取输入文件中的所有序列\n",
    "        with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "            # 读取所有行，去除空行并保留有效序列\n",
    "            dna_sequences = [line.strip() for line in input_file if line.strip()]\n",
    "        \n",
    "        # 2. 批量将T替换为U（DNA→RNA转换）\n",
    "        rna_sequences = []\n",
    "        for seq in dna_sequences:\n",
    "            # 仅替换碱基T为U，保留其他字符（如序列分隔符、注释等，若存在）\n",
    "            rna_seq = seq.replace('T', 'U')\n",
    "            rna_sequences.append(rna_seq)\n",
    "        \n",
    "        # 3. 将转换后的RNA序列写入输出文件\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            # 每行写入一条RNA序列，保持原始文件的分行格式\n",
    "            output_file.write('\\n'.join(rna_sequences))\n",
    "        \n",
    "        print(f\"转换完成！\")\n",
    "        print(f\"输入文件：{input_file_path}\")\n",
    "        print(f\"输出文件：{output_file_path}\")\n",
    "        print(f\"总计转换序列数量：{len(rna_sequences)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"转换过程中出现错误：{str(e)}\")\n",
    "\n",
    "\n",
    "INPUT_FILE = \"all_rna.txt\"    # 输入的原始序列文件（含T的DNA序列）\n",
    "OUTPUT_FILE = \"all_rna_converted.txt\"  # 输出的RNA序列文件（T已替换为U）\n",
    "\n",
    "# 执行转换\n",
    "dna_to_rna(INPUT_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73d3acf1-b96a-4fbb-aa87-8adc6608ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "\"\"\"\n",
    "数据集+DataLoader (支持 BPE)\n",
    "\"\"\"\n",
    "\n",
    "class AptamerDataset(Dataset):\n",
    "    def __init__(self, json_path, smiles_tok_path, rna_tok_path, max_len=128):\n",
    "        with open(json_path) as f:\n",
    "            self.data = json.load(f)\n",
    "        self.smiles_tok = Tokenizer.from_file(smiles_tok_path)\n",
    "        self.rna_tok = Tokenizer.from_file(rna_tok_path)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        smiles = entry[\"smiles\"]\n",
    "        seqs = entry[\"sequences\"]\n",
    "\n",
    "        smiles_ids = self.smiles_tok.encode(smiles).ids[:self.max_len]\n",
    "        seq_ids = [self.rna_tok.encode(s).ids[:self.max_len] for s in seqs]\n",
    "\n",
    "        return {\n",
    "            \"smiles_ids\": torch.tensor(smiles_ids, dtype=torch.long),\n",
    "            \"sequences\": [torch.tensor(s, dtype=torch.long) for s in seq_ids]\n",
    "        }\n",
    "\n",
    "def collate_fn(batch, pad_id=0):\n",
    "    # Pad smiles\n",
    "    max_smi_len = max(len(b[\"smiles_ids\"]) for b in batch)\n",
    "    smiles_tensor = torch.full((len(batch), max_smi_len), pad_id, dtype=torch.long)\n",
    "    for i,b in enumerate(batch):\n",
    "        smiles_tensor[i,:len(b[\"smiles_ids\"])] = b[\"smiles_ids\"]\n",
    "\n",
    "    # keep sequences as list[list[tensor]]\n",
    "    seqs_list = [b[\"sequences\"] for b in batch]\n",
    "    return {\"smiles\": smiles_tensor, \"sequences\": seqs_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08176666-1cce-42a9-901e-0d2fd1fc23e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\"\"\"\n",
    "Transformer Encoder (SMILES) + Transformer Decoder (RNA)\n",
    "\"\"\"\n",
    "\n",
    "class SimpleMolEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead=8, dim_feedforward=512),\n",
    "            num_layers=4\n",
    "        )\n",
    "\n",
    "    def forward(self, smiles_ids):\n",
    "        x = self.embed(smiles_ids).transpose(0,1) # (L,B,D)\n",
    "        x = self.encoder(x) # (L,B,D)\n",
    "        mol_repr = x.mean(0) # (B,D)\n",
    "        return mol_repr\n",
    "\n",
    "class RNASeqDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model, nhead=8, dim_feedforward=512),\n",
    "            num_layers=4\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, tgt_ids, memory):\n",
    "        tgt_emb = self.embed(tgt_ids).transpose(0,1) # (L,B,D)\n",
    "        memory = memory.unsqueeze(0) # (1,B,D)\n",
    "        out = self.decoder(tgt_emb, memory)\n",
    "        logits = self.fc_out(out).transpose(0,1) # (B,L,V)\n",
    "        return logits\n",
    "\n",
    "class Mol2Aptamer(nn.Module):\n",
    "    def __init__(self, smi_vocab_size, rna_vocab_size, d_model=256):\n",
    "        super().__init__()\n",
    "        self.encoder = SimpleMolEncoder(smi_vocab_size, d_model)\n",
    "        self.decoder = RNASeqDecoder(rna_vocab_size, d_model)\n",
    "\n",
    "    def forward(self, smiles_ids, seq_ids):\n",
    "        memory = self.encoder(smiles_ids)\n",
    "        logits = self.decoder(seq_ids, memory)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "558089b1-286e-4e12-824b-66c14e09d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#多实例似然 (MIL loss) \n",
    "\n",
    "def mil_loss(model, smiles_ids, seqs_list, pad_id=0):\n",
    "    \"\"\"\n",
    "    smiles_ids: (B,Lsmi)\n",
    "    seqs_list: list[list[tensor]]\n",
    "    \"\"\"\n",
    "    batch_losses = []\n",
    "    for b, seqs in enumerate(seqs_list):\n",
    "        logps = []\n",
    "        for seq in seqs:\n",
    "            inp = seq[:-1].unsqueeze(0) # (1,L-1)\n",
    "            tgt = seq[1:].unsqueeze(0) # (1,L-1)\n",
    "            logits = model(smiles_ids[b].unsqueeze(0), inp) # (1,L-1,V)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                tgt.reshape(-1),\n",
    "                ignore_index=pad_id,\n",
    "                reduction=\"sum\"\n",
    "            )\n",
    "            logps.append(-loss)\n",
    "        logps = torch.stack(logps)\n",
    "        batch_losses.append(-torch.logsumexp(logps, dim=0))\n",
    "    return torch.stack(batch_losses).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "511448aa-87d1-4b4e-91fa-a56f1f6d8e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练循环\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs=10, lr=1e-4, device=\"cuda\"):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            smiles = batch[\"smiles\"].to(device)\n",
    "            seqs_list = [[s.to(device) for s in seqs] for seqs in batch[\"sequences\"]]\n",
    "\n",
    "            loss = mil_loss(model, smiles, seqs_list, pad_id=0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                smiles = batch[\"smiles\"].to(device)\n",
    "                seqs_list = [[s.to(device) for s in seqs] for seqs in batch[\"sequences\"]]\n",
    "                loss = mil_loss(model, smiles, seqs_list, pad_id=0)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss {train_loss/len(train_loader):.4f} | Val Loss {val_loss/len(val_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cbbd9e3-023a-4c7e-a2c2-31aad450d3b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集拆分完成！总样本数: 238\n",
      "训练集样本数: 191 (80.25%)\n",
      "验证集样本数: 47 (19.75%)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from typing import List, Dict\n",
    "\n",
    "def split_dataset(input_path: str, train_path: str, val_path: str, val_ratio: float = 0.2, seed: int = 42) -> None:\n",
    "    \"\"\"\n",
    "    将JSON格式的数据集随机拆分为训练集和验证集\n",
    "    \n",
    "    Args:\n",
    "        input_path: 输入JSON文件路径\n",
    "        train_path: 训练集输出路径\n",
    "        val_path: 验证集输出路径\n",
    "        val_ratio: 验证集占总数据的比例，默认0.2\n",
    "        seed: 随机种子，保证结果可复现\n",
    "    \"\"\"\n",
    "    # 设置随机种子，确保结果可复现\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # 读取JSON数据\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        data: List[Dict] = json.load(f)\n",
    "    \n",
    "    # 打乱数据顺序\n",
    "    random.shuffle(data)\n",
    "    \n",
    "    # 计算拆分索引\n",
    "    total = len(data)\n",
    "    val_size = int(total * val_ratio)\n",
    "    \n",
    "    # 拆分数据\n",
    "    val_data = data[:val_size]\n",
    "    train_data = data[val_size:]\n",
    "    \n",
    "    # 保存训练集\n",
    "    with open(train_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # 保存验证集\n",
    "    with open(val_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(val_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"数据集拆分完成！总样本数: {total}\")\n",
    "    print(f\"训练集样本数: {len(train_data)} ({len(train_data)/total:.2%})\")\n",
    "    print(f\"验证集样本数: {len(val_data)} ({len(val_data)/total:.2%})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 配置文件路径\n",
    "    INPUT_JSON = \"dataset.json\"\n",
    "    TRAIN_JSON = \"train_dataset.json\"\n",
    "    VAL_JSON = \"val_dataset.json\"\n",
    "    \n",
    "    # 拆分比例设置为20%作为验证集\n",
    "    split_dataset(INPUT_JSON, TRAIN_JSON, VAL_JSON, val_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc7930ff-8fa8-49e4-9402-a2aa4cbba3f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Filtered data: 191 valid entries remaining\n",
      "Filtered data: 47 valid entries remaining\n",
      "Epoch 1/10\n",
      "Train Loss: 98.8628 | Val Loss: 92.6684\n",
      "Epoch 2/10\n",
      "Train Loss: 92.1286 | Val Loss: 89.5070\n",
      "Epoch 3/10\n",
      "Train Loss: 88.4973 | Val Loss: 87.4257\n",
      "Epoch 4/10\n",
      "Train Loss: 85.8195 | Val Loss: 86.3002\n",
      "Epoch 5/10\n",
      "Train Loss: 83.2455 | Val Loss: 84.7849\n",
      "Epoch 6/10\n",
      "Train Loss: 81.1408 | Val Loss: 84.3072\n",
      "Epoch 7/10\n",
      "Train Loss: 79.4558 | Val Loss: 83.4425\n",
      "Epoch 8/10\n",
      "Train Loss: 78.3221 | Val Loss: 83.1255\n",
      "Epoch 9/10\n",
      "Train Loss: 77.7108 | Val Loss: 82.9474\n",
      "Epoch 10/10\n",
      "Train Loss: 77.4760 | Val Loss: 82.8975\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# 训练设置\n",
    "epochs = 10\n",
    "lr = 1e-4\n",
    "batch_size = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "class AptamerDataset(Dataset):\n",
    "    def __init__(self, json_path, smiles_tok_path, rna_tok_path, max_len=128):\n",
    "        with open(json_path) as f:\n",
    "            self.data = json.load(f)\n",
    "        \n",
    "        self.smiles_tok = Tokenizer.from_file(smiles_tok_path)\n",
    "        self.rna_tok = Tokenizer.from_file(rna_tok_path)\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        self.smiles_pad_id = self.smiles_tok.get_vocab()[\"<pad>\"]\n",
    "        self.rna_pad_id = self.rna_tok.get_vocab()[\"<pad>\"]\n",
    "        \n",
    "        self._filter_invalid_data()\n",
    "\n",
    "    def _filter_invalid_data(self):\n",
    "        valid_data = []\n",
    "        for entry in self.data:\n",
    "            if \"smiles\" not in entry or \"sequences\" not in entry:\n",
    "                continue\n",
    "            if not entry[\"smiles\"] or not entry[\"sequences\"]:\n",
    "                continue\n",
    "            valid_seqs = [s for s in entry[\"sequences\"] if s.strip()]\n",
    "            if not valid_seqs:\n",
    "                continue\n",
    "            entry[\"sequences\"] = valid_seqs\n",
    "            valid_data.append(entry)\n",
    "        self.data = valid_data\n",
    "        print(f\"Filtered data: {len(self.data)} valid entries remaining\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        smiles = entry[\"smiles\"]\n",
    "        seqs = entry[\"sequences\"]\n",
    "\n",
    "        # 处理SMILES\n",
    "        smiles_encoded = self.smiles_tok.encode(smiles)\n",
    "        smiles_ids = self._pad_or_truncate(\n",
    "            smiles_encoded.ids, self.max_len, self.smiles_pad_id\n",
    "        )\n",
    "        \n",
    "        # 处理RNA序列\n",
    "        seq_ids_list = []\n",
    "        for s in seqs:\n",
    "            seq_encoded = self.rna_tok.encode(s)\n",
    "            seq_ids = self._pad_or_truncate(\n",
    "                seq_encoded.ids, self.max_len, self.rna_pad_id\n",
    "            )\n",
    "            seq_ids_list.append(seq_ids)\n",
    "\n",
    "        return {\n",
    "            \"smiles_ids\": torch.tensor(smiles_ids, dtype=torch.long),\n",
    "            \"sequences\": torch.tensor(seq_ids_list, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def _pad_or_truncate(self, ids, max_len, pad_id):\n",
    "        if len(ids) > max_len:\n",
    "            return ids[:max_len]\n",
    "        else:\n",
    "            return ids + [pad_id] * (max_len - len(ids))\n",
    "\n",
    "\n",
    "def collate_fn(batch, smiles_pad_id, rna_pad_id):\n",
    "    \"\"\"修复布尔张量处理问题的collate_fn\"\"\"\n",
    "    # 处理SMILES\n",
    "    smiles_list = [item[\"smiles_ids\"] for item in batch]\n",
    "    smiles_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "        smiles_list,\n",
    "        batch_first=True,\n",
    "        padding_value=smiles_pad_id\n",
    "    )\n",
    "    \n",
    "    # 处理RNA序列（修复关键部分）\n",
    "    seqs_list = []\n",
    "    for item in batch:\n",
    "        valid_seqs = []\n",
    "        for seq in item[\"sequences\"]:\n",
    "            # 找到序列中第一个pad_id的位置（修复布尔张量问题）\n",
    "            # 方法1：将布尔张量转换为整数张量\n",
    "            pad_mask = (seq == rna_pad_id).int()  # 转换为整数张量\n",
    "            if pad_mask.sum() > 0:  # 检查是否有padding\n",
    "                # 方法2：使用nonzero找到第一个pad的位置\n",
    "                # valid_len = (seq == rna_pad_id).nonzero(as_tuple=True)[0][0].item()\n",
    "                valid_len = pad_mask.argmax().item()  # 现在可以安全使用argmax了\n",
    "                valid_seq = seq[:valid_len]\n",
    "            else:\n",
    "                valid_seq = seq  # 没有padding，使用完整序列\n",
    "            \n",
    "            # 保留有效长度≥2的序列\n",
    "            if valid_seq.numel() >= 2:\n",
    "                valid_seqs.append(valid_seq)\n",
    "        seqs_list.append(valid_seqs)\n",
    "    \n",
    "    return {\n",
    "        \"smiles\": smiles_padded,\n",
    "        \"seqs_list\": seqs_list\n",
    "    }\n",
    "\n",
    "\n",
    "class Mol2Aptamer(torch.nn.Module):\n",
    "    def __init__(self, smiles_vocab_size, rna_vocab_size, d_model=256):\n",
    "        super().__init__()\n",
    "        self.smiles_embedding = torch.nn.Embedding(smiles_vocab_size, d_model)\n",
    "        self.rna_embedding = torch.nn.Embedding(rna_vocab_size, d_model)\n",
    "        self.pos_embedding = torch.nn.Embedding(512, d_model)\n",
    "        \n",
    "        self.decoder_layer = torch.nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=8,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.decoder = torch.nn.TransformerDecoder(self.decoder_layer, num_layers=3)\n",
    "        self.fc_out = torch.nn.Linear(d_model, rna_vocab_size)\n",
    "\n",
    "    def forward(self, smiles_ids, rna_inp):\n",
    "        batch_size, L_smi = smiles_ids.shape\n",
    "        batch_size, L_rna = rna_inp.shape\n",
    "        \n",
    "        # SMILES编码\n",
    "        smiles_emb = self.smiles_embedding(smiles_ids)\n",
    "        smiles_pos = torch.arange(0, L_smi, device=smiles_ids.device).unsqueeze(0).repeat(batch_size, 1)\n",
    "        smiles_emb += self.pos_embedding(smiles_pos)\n",
    "        \n",
    "        # RNA输入编码\n",
    "        rna_emb = self.rna_embedding(rna_inp)\n",
    "        rna_pos = torch.arange(0, L_rna, device=rna_inp.device).unsqueeze(0).repeat(batch_size, 1)\n",
    "        rna_emb += self.pos_embedding(rna_pos)\n",
    "        \n",
    "        # Transformer解码器\n",
    "        tgt_mask = torch.nn.Transformer.generate_square_subsequent_mask(L_rna, device=rna_inp.device)\n",
    "        decoder_out = self.decoder(tgt=rna_emb, memory=smiles_emb, tgt_mask=tgt_mask)\n",
    "        \n",
    "        # 输出层\n",
    "        logits = self.fc_out(decoder_out)  # (batch_size, L_rna, vocab_size)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def mil_loss(model, smiles_ids, seqs_list, pad_id=0):\n",
    "    batch_losses = []\n",
    "    device = smiles_ids.device\n",
    "    \n",
    "    for b in range(len(seqs_list)):\n",
    "        current_seqs = seqs_list[b]\n",
    "        logps = []\n",
    "        for seq in current_seqs:\n",
    "            if seq.dim() != 1 or seq.numel() < 2:\n",
    "                # print(f\"无效序列，形状: {seq.shape}，长度: {seq.numel()}\")\n",
    "                continue\n",
    "            rna_inp = seq[:-1].unsqueeze(0)\n",
    "            rna_tgt = seq[1:].unsqueeze(0)\n",
    "            logits = model(smiles_ids[b].unsqueeze(0).to(device),rna_inp.to(device))\n",
    "            single_seq_loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)),rna_tgt.reshape(-1).to(device),ignore_index=pad_id,reduction=\"sum\")\n",
    "            logps.append(-single_seq_loss)\n",
    "        if logps:\n",
    "            logps_tensor = torch.stack(logps)\n",
    "            mil_sample_loss = -torch.logsumexp(logps_tensor, dim=0)\n",
    "            batch_losses.append(mil_sample_loss)\n",
    "    if not batch_losses:\n",
    "        return torch.tensor(0.0, device=device, requires_grad=True)\n",
    "    \n",
    "    return torch.stack(batch_losses).mean()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 加载tokenizer和数据集\n",
    "    smiles_tok_path = \"smiles_tokenizer.json\"\n",
    "    rna_tok_path = \"rna_tokenizer.json\"\n",
    "    \n",
    "    smiles_tok = Tokenizer.from_file(smiles_tok_path)\n",
    "    rna_tok = Tokenizer.from_file(rna_tok_path)\n",
    "    smiles_pad_id = smiles_tok.get_vocab()[\"<pad>\"]\n",
    "    rna_pad_id = rna_tok.get_vocab()[\"<pad>\"]\n",
    "    \n",
    "    # 加载数据集\n",
    "    train_dataset = AptamerDataset(\"train_dataset.json\", smiles_tok_path, rna_tok_path)\n",
    "    val_dataset = AptamerDataset(\"val_dataset.json\", smiles_tok_path, rna_tok_path)\n",
    "    \n",
    "    # 数据加载器（传入rna_pad_id）\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda x: collate_fn(x, smiles_pad_id, rna_pad_id)\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda x: collate_fn(x, smiles_pad_id, rna_pad_id)\n",
    "    )\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = Mol2Aptamer(\n",
    "        len(smiles_tok.get_vocab()),\n",
    "        len(rna_tok.get_vocab()),\n",
    "        d_model=256\n",
    "    ).to(device)\n",
    "    \n",
    "    # 优化器和调度器\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            smiles = batch[\"smiles\"].to(device)\n",
    "            seqs_list = batch[\"seqs_list\"]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = mil_loss(model, smiles, seqs_list, pad_id=rna_pad_id)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * smiles.size(0)\n",
    "        \n",
    "        # 验证\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                smiles = batch[\"smiles\"].to(device)\n",
    "                seqs_list = batch[\"seqs_list\"]\n",
    "                loss = mil_loss(model, smiles, seqs_list, pad_id=rna_pad_id)\n",
    "                val_loss += loss.item() * smiles.size(0)\n",
    "        \n",
    "        # 日志\n",
    "        train_avg_loss = train_loss / len(train_dataset)\n",
    "        val_avg_loss = val_loss / len(val_dataset)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"Train Loss: {train_avg_loss:.4f} | Val Loss: {val_avg_loss:.4f}\")\n",
    "        \n",
    "        scheduler.step()\n",
    "        torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pth\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c84daf-f00e-4dce-8f1c-a820de337410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import numpy as np\n",
    "\n",
    "# 训练设置\n",
    "epochs = 500\n",
    "lr = 5e-5\n",
    "batch_size = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "class AptamerDataset(Dataset):\n",
    "    def __init__(self, json_path, smiles_tok_path, rna_tok_path, max_len=128):\n",
    "        with open(json_path) as f:\n",
    "            self.data = json.load(f)\n",
    "        \n",
    "        # 假设Tokenizer类已经正确导入\n",
    "        self.smiles_tok = Tokenizer.from_file(smiles_tok_path)\n",
    "        self.rna_tok = Tokenizer.from_file(rna_tok_path)\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        self.smiles_pad_id = self.smiles_tok.get_vocab()[\"<pad>\"]\n",
    "        self.rna_pad_id = self.rna_tok.get_vocab()[\"<pad>\"]\n",
    "        \n",
    "        self._filter_invalid_data()\n",
    "\n",
    "    def _filter_invalid_data(self):\n",
    "        valid_data = []\n",
    "        for entry in self.data:\n",
    "            if \"smiles\" not in entry or \"sequences\" not in entry:\n",
    "                continue\n",
    "            if not entry[\"smiles\"] or not entry[\"sequences\"]:\n",
    "                continue\n",
    "            valid_seqs = [s for s in entry[\"sequences\"] if s.strip()]\n",
    "            if not valid_seqs:\n",
    "                continue\n",
    "            entry[\"sequences\"] = valid_seqs\n",
    "            valid_data.append(entry)\n",
    "        self.data = valid_data\n",
    "        print(f\"Filtered data: {len(self.data)} valid entries remaining\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        smiles = entry[\"smiles\"]\n",
    "        seqs = entry[\"sequences\"]\n",
    "\n",
    "        # 处理SMILES\n",
    "        smiles_encoded = self.smiles_tok.encode(smiles)\n",
    "        smiles_ids = self._pad_or_truncate(\n",
    "            smiles_encoded.ids, self.max_len, self.smiles_pad_id\n",
    "        )\n",
    "        \n",
    "        # 处理RNA序列\n",
    "        seq_ids_list = []\n",
    "        for s in seqs:\n",
    "            seq_encoded = self.rna_tok.encode(s)\n",
    "            seq_ids = self._pad_or_truncate(\n",
    "                seq_encoded.ids, self.max_len, self.rna_pad_id\n",
    "            )\n",
    "            seq_ids_list.append(seq_ids)\n",
    "\n",
    "        return {\n",
    "            \"smiles_ids\": torch.tensor(smiles_ids, dtype=torch.long),\n",
    "            \"sequences\": torch.tensor(seq_ids_list, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def _pad_or_truncate(self, ids, max_len, pad_id):\n",
    "        if len(ids) > max_len:\n",
    "            return ids[:max_len]\n",
    "        else:\n",
    "            return ids + [pad_id] * (max_len - len(ids))\n",
    "\n",
    "\n",
    "def collate_fn(batch, smiles_pad_id, rna_pad_id):\n",
    "    \"\"\"修复布尔张量处理问题的collate_fn\"\"\"\n",
    "    # 处理SMILES\n",
    "    smiles_list = [item[\"smiles_ids\"] for item in batch]\n",
    "    smiles_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "        smiles_list,\n",
    "        batch_first=True,\n",
    "        padding_value=smiles_pad_id\n",
    "    )\n",
    "    \n",
    "    # 处理RNA序列\n",
    "    seqs_list = []\n",
    "    for item in batch:\n",
    "        valid_seqs = []\n",
    "        for seq in item[\"sequences\"]:\n",
    "            # 找到序列中第一个pad_id的位置\n",
    "            pad_mask = (seq == rna_pad_id).int()\n",
    "            if pad_mask.sum() > 0:\n",
    "                valid_len = pad_mask.argmax().item()\n",
    "                valid_seq = seq[:valid_len]\n",
    "            else:\n",
    "                valid_seq = seq  # 没有padding，使用完整序列\n",
    "            \n",
    "            # 保留有效长度≥2的序列\n",
    "            if valid_seq.numel() >= 2:\n",
    "                valid_seqs.append(valid_seq)\n",
    "        seqs_list.append(valid_seqs)\n",
    "    \n",
    "    return {\n",
    "        \"smiles\": smiles_padded,\n",
    "        \"seqs_list\": seqs_list\n",
    "    }\n",
    "\n",
    "\n",
    "class Mol2Aptamer(torch.nn.Module):\n",
    "    def __init__(self, smiles_vocab_size, rna_vocab_size, d_model=256):\n",
    "        super().__init__()\n",
    "        self.smiles_embedding = torch.nn.Embedding(smiles_vocab_size, d_model)\n",
    "        self.rna_embedding = torch.nn.Embedding(rna_vocab_size, d_model)\n",
    "        self.pos_embedding = torch.nn.Embedding(512, d_model)\n",
    "        \n",
    "        # 添加SMILES编码器\n",
    "        self.encoder = torch.nn.TransformerEncoder(\n",
    "            torch.nn.TransformerEncoderLayer(d_model=d_model, nhead=8, batch_first=True),\n",
    "            num_layers=2\n",
    "        )\n",
    "        self.decoder_layer = torch.nn.TransformerDecoderLayer(d_model=d_model, nhead=8, batch_first=True)\n",
    "        self.decoder = torch.nn.TransformerDecoder(self.decoder_layer, num_layers=3)\n",
    "        self.fc_out = torch.nn.Linear(d_model, rna_vocab_size)\n",
    "\n",
    "    def forward(self, smiles_ids, rna_inp):\n",
    "        batch_size, L_smi = smiles_ids.shape\n",
    "        batch_size, L_rna = rna_inp.shape\n",
    "        \n",
    "        # SMILES编码（使用Encoder）\n",
    "        smiles_emb = self.smiles_embedding(smiles_ids)\n",
    "        smiles_pos = torch.arange(0, L_smi, device=smiles_ids.device).unsqueeze(0).repeat(batch_size, 1)\n",
    "        smiles_emb += self.pos_embedding(smiles_pos)\n",
    "        smiles_enc = self.encoder(smiles_emb)  # 新增Encoder输出\n",
    "        \n",
    "        # RNA输入编码（不变）\n",
    "        rna_emb = self.rna_embedding(rna_inp)\n",
    "        rna_pos = torch.arange(0, L_rna, device=rna_inp.device).unsqueeze(0).repeat(batch_size, 1)\n",
    "        rna_emb += self.pos_embedding(rna_pos)\n",
    "        \n",
    "        # Decoder接收Encoder输出作为memory\n",
    "        tgt_mask = torch.nn.Transformer.generate_square_subsequent_mask(L_rna, device=rna_inp.device)\n",
    "        decoder_out = self.decoder(tgt=rna_emb, memory=smiles_enc, tgt_mask=tgt_mask)  # 改用Encoder输出\n",
    "        \n",
    "        logits = self.fc_out(decoder_out)\n",
    "        return logits\n",
    "\n",
    "def mil_loss(model, smiles_ids, seqs_list, pad_id=0):\n",
    "    batch_losses = []\n",
    "    device = smiles_ids.device\n",
    "    \n",
    "    for b in range(len(seqs_list)):\n",
    "        current_seqs = seqs_list[b]\n",
    "        logps = []\n",
    "        \n",
    "        for seq in current_seqs:\n",
    "            if seq.dim() != 1 or seq.numel() < 2:\n",
    "                print(f\"Warning: 跳过无效序列，形状: {seq.shape}，长度: {seq.numel()}\")\n",
    "                continue\n",
    "            \n",
    "            rna_inp = seq[:-1].unsqueeze(0)\n",
    "            rna_tgt = seq[1:].unsqueeze(0)\n",
    "            \n",
    "            logits = model(\n",
    "                smiles_ids[b].unsqueeze(0).to(device),\n",
    "                rna_inp.to(device)\n",
    "            )\n",
    "            \n",
    "            single_seq_loss = F.cross_entropy(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                rna_tgt.reshape(-1).to(device),\n",
    "                ignore_index=pad_id,\n",
    "                reduction=\"sum\"\n",
    "            )\n",
    "            logps.append(-single_seq_loss)\n",
    "        \n",
    "        if logps:\n",
    "            logps_tensor = torch.stack(logps)\n",
    "            mil_sample_loss = -torch.logsumexp(logps_tensor, dim=0)\n",
    "            batch_losses.append(mil_sample_loss)\n",
    "    \n",
    "    if not batch_losses:\n",
    "        return torch.tensor(0.0, device=device, requires_grad=True)\n",
    "    \n",
    "    return torch.stack(batch_losses).mean()\n",
    "\n",
    "\n",
    "def calculate_metrics(model, smiles_ids, seqs_list, pad_id=0):\n",
    "    \"\"\"计算验证指标\"\"\"\n",
    "    device = smiles_ids.device\n",
    "    total_tokens = 0\n",
    "    correct_tokens = 0\n",
    "    total_sequences = 0\n",
    "    correct_sequences = 0\n",
    "    total_loss = 0.0\n",
    "    sequence_lengths = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for b in range(len(seqs_list)):\n",
    "            current_seqs = seqs_list[b]\n",
    "            \n",
    "            for seq in current_seqs:\n",
    "                if seq.dim() != 1 or seq.numel() < 2:\n",
    "                    continue\n",
    "                \n",
    "                total_sequences += 1\n",
    "                seq_len = len(seq) - 1  # 因为我们用前n-1个预测第n个\n",
    "                sequence_lengths.append(seq_len)\n",
    "                \n",
    "                rna_inp = seq[:-1].unsqueeze(0).to(device)\n",
    "                rna_tgt = seq[1:].unsqueeze(0).to(device)\n",
    "                \n",
    "                # 获取模型预测\n",
    "                logits = model(\n",
    "                    smiles_ids[b].unsqueeze(0).to(device),\n",
    "                    rna_inp\n",
    "                )\n",
    "                \n",
    "                # 计算损失\n",
    "                loss = F.cross_entropy(\n",
    "                    logits.reshape(-1, logits.size(-1)),\n",
    "                    rna_tgt.reshape(-1),\n",
    "                    ignore_index=pad_id,\n",
    "                    reduction=\"sum\"\n",
    "                )\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # 计算token级准确率\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "                mask = (rna_tgt != pad_id).flatten()\n",
    "                \n",
    "                # 计算匹配的token\n",
    "                correct = (preds.flatten()[mask] == rna_tgt.flatten()[mask]).sum().item()\n",
    "                total = mask.sum().item()\n",
    "                \n",
    "                correct_tokens += correct\n",
    "                total_tokens += total\n",
    "                \n",
    "                # 计算序列级准确率（完全匹配）\n",
    "                if total > 0 and correct == total:\n",
    "                    correct_sequences += 1\n",
    "    \n",
    "    # 计算指标\n",
    "    metrics = {\n",
    "        \"token_accuracy\": correct_tokens / total_tokens if total_tokens > 0 else 0.0,\n",
    "        \"sequence_accuracy\": correct_sequences / total_sequences if total_sequences > 0 else 0.0,\n",
    "        \"perplexity\": np.exp(total_loss / total_tokens) if total_tokens > 0 else 0.0,\n",
    "        \"avg_sequence_length\": np.mean(sequence_lengths) if sequence_lengths else 0.0\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 加载tokenizer和数据集\n",
    "    smiles_tok_path = \"/root/autodl-tmp/rna/smiles_tokenizer.json\"\n",
    "    rna_tok_path = \"/root/autodl-tmp/rna/rna_tokenizer.json\"\n",
    "    \n",
    "    # 假设Tokenizer类已经正确导入\n",
    "    smiles_tok = Tokenizer.from_file(smiles_tok_path)\n",
    "    rna_tok = Tokenizer.from_file(rna_tok_path)\n",
    "    smiles_pad_id = smiles_tok.get_vocab()[\"<pad>\"]\n",
    "    rna_pad_id = rna_tok.get_vocab()[\"<pad>\"]\n",
    "    \n",
    "    # 加载数据集\n",
    "    train_dataset = AptamerDataset(\"/root/autodl-tmp/rna/train_dataset.json\", smiles_tok_path, rna_tok_path)\n",
    "    val_dataset = AptamerDataset(\"/root/autodl-tmp/rna/val_dataset.json\", smiles_tok_path, rna_tok_path)\n",
    "    \n",
    "    # 数据加载器\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda x: collate_fn(x, smiles_pad_id, rna_pad_id)\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda x: collate_fn(x, smiles_pad_id, rna_pad_id)\n",
    "    )\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = Mol2Aptamer(\n",
    "        len(smiles_tok.get_vocab()),\n",
    "        len(rna_tok.get_vocab()),\n",
    "        d_model=256\n",
    "    ).to(device)\n",
    "    \n",
    "    # 优化器和调度器\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            smiles = batch[\"smiles\"].to(device)\n",
    "            seqs_list = batch[\"seqs_list\"]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = mil_loss(model, smiles, seqs_list, pad_id=rna_pad_id)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * smiles.size(0)\n",
    "        \n",
    "        # 验证\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_metrics = {\n",
    "            \"token_accuracy\": [],\n",
    "            \"sequence_accuracy\": [],\n",
    "            \"perplexity\": [],\n",
    "            \"avg_sequence_length\": []\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                smiles = batch[\"smiles\"].to(device)\n",
    "                seqs_list = batch[\"seqs_list\"]\n",
    "                \n",
    "                # 计算损失\n",
    "                loss = mil_loss(model, smiles, seqs_list, pad_id=rna_pad_id)\n",
    "                val_loss += loss.item() * smiles.size(0)\n",
    "                \n",
    "                # 计算评估指标\n",
    "                metrics = calculate_metrics(model, smiles, seqs_list, pad_id=rna_pad_id)\n",
    "                \n",
    "                # 收集指标\n",
    "                for key, value in metrics.items():\n",
    "                    all_metrics[key].append(value)\n",
    "        \n",
    "        # 计算平均指标\n",
    "        avg_metrics = {\n",
    "            key: np.mean(values) if values else 0.0 \n",
    "            for key, values in all_metrics.items()\n",
    "        }\n",
    "        \n",
    "        # 日志\n",
    "        train_avg_loss = train_loss / len(train_dataset)\n",
    "        val_avg_loss = val_loss / len(val_dataset)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        print(f\"Train Loss: {train_avg_loss:.4f} | Val Loss: {val_avg_loss:.4f}\")\n",
    "        print(f\"Token Accuracy: {avg_metrics['token_accuracy']:.4f}\")\n",
    "        print(f\"Sequence Accuracy: {avg_metrics['sequence_accuracy']:.4f}\")\n",
    "        print(f\"Perplexity: {avg_metrics['perplexity']:.4f}\")\n",
    "        print(f\"Average Sequence Length: {avg_metrics['avg_sequence_length']:.2f}\")\n",
    "        \n",
    "        scheduler.step()\n",
    "        torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pth\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d50c5063-f612-4144-8798-2ff504cf69ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9702fe79-abc5-425a-a48a-9e940834d5c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv_mol2aptamer (Python 3.12.9)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/home/zhangliqin/RNA-Factory/.venv_mol2aptamer/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#推理函数\n",
    "#输入SMILES字符串，输出若干条候选aptamer序列\n",
    "#贪心解码 +Top-k 采样 +Top-p (nucleus) 采样 +多样性控制（temperature）\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "def generate_aptamers(\n",
    "    model, smiles, smiles_tokenizer, rna_tokenizer,\n",
    "    max_len=80, num_return=5,\n",
    "    strategy=\"topk\", top_k=5, top_p=0.9, temperature=1.0,\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    \"\"\"\n",
    "    model: 已训练的 Mol2Aptamer\n",
    "    smiles: 输入 SMILES 字符串\n",
    "    smiles_tokenizer: BPE tokenizer for SMILES\n",
    "    rna_tokenizer: BPE tokenizer for RNA\n",
    "    strategy: \"greedy\", \"topk\", \"topp\"\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Encode SMILES\n",
    "    smi_ids = smiles_tokenizer.encode(smiles).ids\n",
    "    smi_ids = torch.tensor(smi_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # BOS token id\n",
    "    bos_id = rna_tokenizer.token_to_id(\"<bos>\")\n",
    "    eos_id = rna_tokenizer.token_to_id(\"<eos>\")\n",
    "\n",
    "    results = []\n",
    "    for _ in range(num_return):\n",
    "        generated = [bos_id]\n",
    "        memory = model.encoder(smi_ids)  # (1,D)\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            inp = torch.tensor([generated], dtype=torch.long).to(device)\n",
    "            logits = model.decoder(inp, memory)[:, -1, :]  # (1,V)\n",
    "            logits = logits / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            if strategy == \"greedy\":\n",
    "                next_id = torch.argmax(probs, dim=-1).item()\n",
    "            elif strategy == \"topk\":\n",
    "                topk_probs, topk_ids = torch.topk(probs, k=top_k)\n",
    "                idx = torch.multinomial(topk_probs, 1).item()\n",
    "                next_id = topk_ids[0, idx].item()\n",
    "            elif strategy == \"topp\":\n",
    "                sorted_probs, sorted_ids = torch.sort(probs, descending=True)\n",
    "                cumprobs = torch.cumsum(sorted_probs, dim=-1)\n",
    "                mask = cumprobs <= top_p\n",
    "                cutoff = mask.sum().item()\n",
    "                filtered_probs = sorted_probs[:, :cutoff]\n",
    "                filtered_ids = sorted_ids[:, :cutoff]\n",
    "                idx = torch.multinomial(filtered_probs, 1).item()\n",
    "                next_id = filtered_ids[0, idx].item()\n",
    "            else:\n",
    "                raise ValueError(\"Unknown strategy\")\n",
    "\n",
    "            if next_id == eos_id:\n",
    "                break\n",
    "            generated.append(next_id)\n",
    "\n",
    "        # Decode\n",
    "        seq = rna_tokenizer.decode(generated)\n",
    "        results.append(seq)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09b0d5c9-a2db-4e97-a2de-1d2fe05cd119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "def generate_aptamers(\n",
    "    model, smiles, smiles_tokenizer, rna_tokenizer,\n",
    "    max_len=80, num_return=5,\n",
    "    strategy=\"topk\", top_k=5, top_p=0.9, temperature=1.0,\n",
    "    device=None\n",
    "):\n",
    "    \"\"\"\n",
    "    生成Aptamer序列的推理函数，支持多种采样策略\n",
    "    \n",
    "    Args:\n",
    "        model: 已训练的 Mol2Aptamer 模型\n",
    "        smiles: 输入的SMILES字符串\n",
    "        smiles_tokenizer: SMILES的BPE分词器\n",
    "        rna_tokenizer: RNA的BPE分词器（基于提供的rna_tokenizer.json）\n",
    "        max_len: 生成序列的最大长度\n",
    "        num_return: 生成的候选序列数量\n",
    "        strategy: 采样策略，可选\"greedy\", \"topk\", \"topp\"\n",
    "        top_k: Top-k采样的候选数量（仅用于strategy=\"topk\"）\n",
    "        top_p: Top-p采样的累积概率阈值（仅用于strategy=\"topp\"）\n",
    "        temperature: 多样性控制参数，值越大多样性越高\n",
    "        device: 推理设备，默认为模型所在设备\n",
    "    \n",
    "    Returns:\n",
    "        list: 去重后的候选Aptamer序列列表\n",
    "    \"\"\"\n",
    "    # 设备自动选择与模型部署\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device if hasattr(model, 'parameters') else torch.device('cpu')\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # 验证RNA分词器特殊token（基于rna_tokenizer.json中的定义）\n",
    "    required_tokens = [\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"]\n",
    "    for token in required_tokens:\n",
    "        if rna_tokenizer.token_to_id(token) is None:\n",
    "            raise ValueError(f\"RNA tokenizer missing required token: {token}\")\n",
    "    \n",
    "    bos_id = rna_tokenizer.token_to_id(\"<bos>\")\n",
    "    eos_id = rna_tokenizer.token_to_id(\"<eos>\")\n",
    "    pad_id = rna_tokenizer.token_to_id(\"<pad>\")\n",
    "    unk_id = rna_tokenizer.token_to_id(\"<unk>\")  # 新增unk token处理\n",
    "    \n",
    "    # 编码SMILES\n",
    "    try:\n",
    "        smi_encoded = smiles_tokenizer.encode(smiles)\n",
    "        smi_ids = torch.tensor(smi_encoded.ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to encode SMILES: {str(e)}\")\n",
    "    \n",
    "    results = []\n",
    "    with torch.no_grad():  # 关闭梯度计算，加速推理\n",
    "        # 预计算encoder输出（所有生成共享，提高效率）\n",
    "        memory = model.encoder(smi_ids)  # (1, D)\n",
    "        \n",
    "        for _ in range(num_return):\n",
    "            generated = [bos_id]\n",
    "            has_unk = False  # 跟踪是否生成了未知token\n",
    "            \n",
    "            for _ in range(max_len):\n",
    "                # 准备解码器输入\n",
    "                inp = torch.tensor([generated], dtype=torch.long).to(device)\n",
    "                \n",
    "                # 用 forward 而不是直接 decoder\n",
    "                logits = model(smi_ids, inp)[:, -1, :]  # (1, V)\n",
    "                \n",
    "                # 应用温度调节\n",
    "                if temperature <= 0:\n",
    "                    raise ValueError(\"Temperature must be positive\")\n",
    "                logits = logits / temperature\n",
    "                \n",
    "                # 根据不同策略采样下一个token\n",
    "                if strategy == \"greedy\":\n",
    "                    next_id = torch.argmax(logits, dim=-1).item()\n",
    "                \n",
    "                elif strategy == \"topk\":\n",
    "                    if top_k <= 0 or top_k > logits.size(-1):\n",
    "                        raise ValueError(f\"Invalid top_k value: {top_k}\")\n",
    "                    topk_probs, topk_ids = torch.topk(logits, k=top_k)\n",
    "                    topk_probs = F.softmax(topk_probs, dim=-1)  # 重新归一化\n",
    "                    idx = torch.multinomial(topk_probs, 1).item()\n",
    "                    next_id = topk_ids[0, idx].item()\n",
    "                \n",
    "                elif strategy == \"topp\":\n",
    "                    if top_p <= 0 or top_p > 1:\n",
    "                        raise ValueError(f\"top_p must be in (0, 1], got {top_p}\")\n",
    "                    \n",
    "                    # 按概率排序\n",
    "                    sorted_logits, sorted_ids = torch.sort(logits, descending=True)\n",
    "                    sorted_probs = F.softmax(sorted_logits, dim=-1)\n",
    "                    \n",
    "                    # 计算累积概率\n",
    "                    cum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "                    \n",
    "                    # 找到满足累积概率 <= top_p 的所有token\n",
    "                    cutoff = torch.sum(cum_probs <= top_p).item()\n",
    "                    cutoff = max(1, cutoff)  # 确保至少保留一个token\n",
    "                    \n",
    "                    # 从筛选出的token中采样\n",
    "                    filtered_probs = sorted_probs[:, :cutoff]\n",
    "                    filtered_ids = sorted_ids[:, :cutoff]\n",
    "                    idx = torch.multinomial(filtered_probs, 1).item()\n",
    "                    next_id = filtered_ids[0, idx].item()\n",
    "                \n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown sampling strategy: {strategy}. Choose from 'greedy', 'topk', 'topp'\")\n",
    "                \n",
    "                # 检查是否生成未知token\n",
    "                if next_id == unk_id:\n",
    "                    has_unk = True\n",
    "                \n",
    "                # 检查是否达到终止符\n",
    "                if next_id == eos_id:\n",
    "                    break\n",
    "                \n",
    "                generated.append(next_id)\n",
    "            \n",
    "            # 解码并处理生成的序列\n",
    "            # 过滤特殊token（保留原始序列中的有效碱基token）\n",
    "            filtered_ids = [id for id in generated if id not in [bos_id, eos_id, pad_id, unk_id]]\n",
    "            \n",
    "            # 解码为字符串（适配BPE分词器的合并规则）\n",
    "            try:\n",
    "                # 使用分词器原生解码方法，确保BPE合并规则正确应用\n",
    "                seq = rna_tokenizer.decode(filtered_ids, skip_special_tokens=True)\n",
    "                \n",
    "                # 过滤包含未知token或空的序列\n",
    "                if seq and not has_unk and seq not in results:\n",
    "                    results.append(seq)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to decode sequence: {str(e)}\")\n",
    "    \n",
    "    # 确保返回指定数量的序列（如果去重后不足）\n",
    "    while len(results) < num_return:\n",
    "        if not results:\n",
    "            results.append(\"\")\n",
    "        else:\n",
    "            results.append(results[-1])\n",
    "    \n",
    "    return results[:num_return]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "388614a8-5452-4900-970b-0d32b5815e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# 加载训练好的模型（假设模型已保存为pth文件）\n",
    "model = torch.load(\"/root/autodl-tmp/model_epoch_59.pth\")  # 替换为你的模型路径\n",
    "\n",
    "# 加载SMILES和RNA的分词器\n",
    "smiles_tokenizer = Tokenizer.from_file(\"/root/autodl-tmp/rna/smiles_tokenizer.json\")  # 替换为你的SMILES分词器路径\n",
    "rna_tokenizer = Tokenizer.from_file(\"/root/autodl-tmp/rna/rna_tokenizer.json\")       # 替换为你的RNA分词器路径（即提供的json文件）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "383d9ee9-3428-4ddc-9b94-1302f448ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mol2Aptamer(torch.nn.Module):\n",
    "    def __init__(self, smiles_vocab_size, rna_vocab_size, d_model=256):\n",
    "        super().__init__()\n",
    "        self.smiles_embedding = torch.nn.Embedding(smiles_vocab_size, d_model)\n",
    "        self.rna_embedding = torch.nn.Embedding(rna_vocab_size, d_model)\n",
    "        self.pos_embedding = torch.nn.Embedding(512, d_model)\n",
    "\n",
    "        self.decoder_layer = torch.nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=8,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.decoder = torch.nn.TransformerDecoder(self.decoder_layer, num_layers=3)\n",
    "        self.fc_out = torch.nn.Linear(d_model, rna_vocab_size)\n",
    "\n",
    "    def forward(self, smiles_ids, rna_inp):\n",
    "        batch_size, L_smi = smiles_ids.shape\n",
    "        batch_size, L_rna = rna_inp.shape\n",
    "\n",
    "        smiles_emb = self.smiles_embedding(smiles_ids)\n",
    "        smiles_pos = torch.arange(0, L_smi, device=smiles_ids.device).unsqueeze(0).repeat(batch_size, 1)\n",
    "        smiles_emb += self.pos_embedding(smiles_pos)\n",
    "\n",
    "        rna_emb = self.rna_embedding(rna_inp)\n",
    "        rna_pos = torch.arange(0, L_rna, device=rna_inp.device).unsqueeze(0).repeat(batch_size, 1)\n",
    "        rna_emb += self.pos_embedding(rna_pos)\n",
    "\n",
    "        tgt_mask = torch.nn.Transformer.generate_square_subsequent_mask(L_rna, device=rna_inp.device)\n",
    "        decoder_out = self.decoder(tgt=rna_emb, memory=smiles_emb, tgt_mask=tgt_mask)\n",
    "\n",
    "        logits = self.fc_out(decoder_out)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11f7bd70-69e2-4c4a-8f77-1df4bad5940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mol2Aptamer(torch.nn.Module):\n",
    "    def __init__(self, smiles_vocab_size, rna_vocab_size, d_model=256):\n",
    "        super().__init__()\n",
    "        self.smiles_embedding = torch.nn.Embedding(smiles_vocab_size, d_model)\n",
    "        self.rna_embedding = torch.nn.Embedding(rna_vocab_size, d_model)\n",
    "        self.pos_embedding = torch.nn.Embedding(512, d_model)\n",
    "\n",
    "        self.decoder_layer = torch.nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=8,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.decoder = torch.nn.TransformerDecoder(self.decoder_layer, num_layers=3)\n",
    "        self.fc_out = torch.nn.Linear(d_model, rna_vocab_size)\n",
    "\n",
    "    def encoder(self, smiles_ids):  \n",
    "        batch_size, L_smi = smiles_ids.shape\n",
    "        smiles_emb = self.smiles_embedding(smiles_ids)\n",
    "        smiles_pos = torch.arange(0, L_smi, device=smiles_ids.device).unsqueeze(0).repeat(batch_size, 1)\n",
    "        smiles_emb += self.pos_embedding(smiles_pos)\n",
    "        return smiles_emb\n",
    "\n",
    "    def forward(self, smiles_ids, rna_inp):\n",
    "        smiles_emb = self.encoder(smiles_ids)\n",
    "\n",
    "        batch_size, L_rna = rna_inp.shape\n",
    "        rna_emb = self.rna_embedding(rna_inp)\n",
    "        rna_pos = torch.arange(0, L_rna, device=rna_inp.device).unsqueeze(0).repeat(batch_size, 1)\n",
    "        rna_emb += self.pos_embedding(rna_pos)\n",
    "\n",
    "        tgt_mask = torch.nn.Transformer.generate_square_subsequent_mask(L_rna, device=rna_inp.device)\n",
    "        decoder_out = self.decoder(tgt=rna_emb, memory=smiles_emb, tgt_mask=tgt_mask)\n",
    "\n",
    "        logits = self.fc_out(decoder_out)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "285286e9-d4d7-407c-81e8-bd3cac1a4a81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始权重中的嵌入层相关键：\n",
      "- smiles_embedding.weight: 形状 torch.Size([188, 256])\n",
      "- rna_embedding.weight: 形状 torch.Size([100, 256])\n",
      "- pos_embedding.weight: 形状 torch.Size([512, 256])\n",
      "--------------------------------------------------\n",
      "跳过冗余键：decoder_layer.self_attn.in_proj_weight\n",
      "跳过冗余键：decoder_layer.self_attn.in_proj_bias\n",
      "跳过冗余键：decoder_layer.self_attn.out_proj.weight\n",
      "跳过冗余键：decoder_layer.self_attn.out_proj.bias\n",
      "跳过冗余键：decoder_layer.multihead_attn.in_proj_weight\n",
      "跳过冗余键：decoder_layer.multihead_attn.in_proj_bias\n",
      "跳过冗余键：decoder_layer.multihead_attn.out_proj.weight\n",
      "跳过冗余键：decoder_layer.multihead_attn.out_proj.bias\n",
      "跳过冗余键：decoder_layer.linear1.weight\n",
      "跳过冗余键：decoder_layer.linear1.bias\n",
      "跳过冗余键：decoder_layer.linear2.weight\n",
      "跳过冗余键：decoder_layer.linear2.bias\n",
      "跳过冗余键：decoder_layer.norm1.weight\n",
      "跳过冗余键：decoder_layer.norm1.bias\n",
      "跳过冗余键：decoder_layer.norm2.weight\n",
      "跳过冗余键：decoder_layer.norm2.bias\n",
      "跳过冗余键：decoder_layer.norm3.weight\n",
      "跳过冗余键：decoder_layer.norm3.bias\n",
      "权重加载成功！\n",
      "模型嵌入层输出维度：256\n",
      "SMILES嵌入层权重形状：torch.Size([188, 256])\n",
      "RNA嵌入层权重形状：torch.Size([100, 256])\n",
      "\n",
      "候选Aptamer序列：\n",
      "1. Ġ GGU AAU AC GCA GA CGU GAGG GAU GCA CU CGG AU GCGU AGG GG GUU GAU CA\n",
      "2. Ġ A GCA GCA CA GA GGU CAU CUU GAU CU CGG CUU GAU AGG GU CGU CC GUAA CU CC C\n",
      "3. Ġ CC GG AA CU A CUU CA CGU AC GACU GU CACA CCU GAGG GGU CGU AA CAA GU GGU AU GCGU\n",
      "4. Ġ CC GG GGU GG AA AC GAU CU CAU CC GG GUU GU\n",
      "5. Ġ A GCA GCA CA GA GGU CA GAU GCA CU CGG ACC CC AUU CU CC UU CC AUCC CU CAU CC GUCC ACC CU AU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# --------------------------\n",
    "# 1. 重新定义模型（确保嵌入层维度=256）\n",
    "# --------------------------\n",
    "class Mol2Aptamer(nn.Module):\n",
    "    def __init__(self, smiles_vocab_size, rna_vocab_size, d_model=256, nhead=8, num_encoder_layers=2, num_decoder_layers=3):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model  # 显式保存模型维度，便于验证\n",
    "        \n",
    "        # Encoder：2层 TransformerEncoder（d_model=256）\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=nn.TransformerEncoderLayer(\n",
    "                d_model=d_model,       # 注意力层期望的维度=256\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=2048,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=num_encoder_layers\n",
    "        )\n",
    "        \n",
    "        # Decoder：3层 TransformerDecoder（d_model=256）\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            decoder_layer=nn.TransformerDecoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=2048,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=num_decoder_layers\n",
    "        )\n",
    "        \n",
    "        # 嵌入层：明确设置输出维度=d_model=256\n",
    "        self.smiles_embedding = nn.Embedding(smiles_vocab_size, d_model)  # 输出 (seq_len, 256)\n",
    "        self.rna_embedding = nn.Embedding(rna_vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Embedding(512, d_model)  # 位置编码维度也=256\n",
    "        \n",
    "        # 输出层\n",
    "        self.fc_out = nn.Linear(d_model, rna_vocab_size)\n",
    "\n",
    "    def forward(self, smiles_ids, rna_inp):\n",
    "        batch_size, seq_len_smi = smiles_ids.shape\n",
    "        batch_size, seq_len_rna = rna_inp.shape\n",
    "        device = smiles_ids.device\n",
    "        \n",
    "        # --------------------------\n",
    "        # 关键：验证嵌入层输出维度（确保=256）\n",
    "        # --------------------------\n",
    "        smiles_emb = self.smiles_embedding(smiles_ids)\n",
    "        assert smiles_emb.shape[-1] == self.d_model, \\\n",
    "            f\"嵌入层输出维度错误：期望 {self.d_model}，实际 {smiles_emb.shape[-1]}\"\n",
    "        \n",
    "        # 叠加位置编码（维度需与嵌入层一致）\n",
    "        smiles_pos = torch.arange(seq_len_smi, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
    "        smiles_emb += self.pos_embedding(smiles_pos)\n",
    "        assert smiles_emb.shape[-1] == self.d_model, \\\n",
    "            f\"位置编码后维度错误：期望 {self.d_model}，实际 {smiles_emb.shape[-1]}\"\n",
    "        \n",
    "        # Encoder前向（输入维度=256，匹配注意力层期望）\n",
    "        memory = self.encoder(smiles_emb)\n",
    "        \n",
    "        # RNA嵌入与位置编码（同样验证维度）\n",
    "        rna_emb = self.rna_embedding(rna_inp)\n",
    "        assert rna_emb.shape[-1] == self.d_model, \\\n",
    "            f\"RNA嵌入层输出维度错误：期望 {self.d_model}，实际 {rna_emb.shape[-1]}\"\n",
    "        rna_pos = torch.arange(seq_len_rna, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
    "        rna_emb += self.pos_embedding(rna_pos)\n",
    "        \n",
    "        # Decoder前向\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq_len_rna, device=device)\n",
    "        decoder_out = self.decoder(tgt=rna_emb, memory=memory, tgt_mask=tgt_mask)\n",
    "        \n",
    "        # 输出层\n",
    "        logits = self.fc_out(decoder_out)\n",
    "        return logits\n",
    "\n",
    "# --------------------------\n",
    "# 2. 加载分词器 + 初始化模型（显式确认嵌入层维度）\n",
    "# --------------------------\n",
    "smiles_tokenizer = Tokenizer.from_file(\"/root/autodl-tmp/rna/smiles_tokenizer.json\")\n",
    "rna_tokenizer = Tokenizer.from_file(\"/root/autodl-tmp/rna/rna_tokenizer.json\")\n",
    "\n",
    "# 词汇表大小\n",
    "smiles_vocab_size = len(smiles_tokenizer.get_vocab())\n",
    "rna_vocab_size = len(rna_tokenizer.get_vocab())\n",
    "\n",
    "# 设备与模型初始化（d_model=256，嵌入层输出维度=256）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Mol2Aptamer(\n",
    "    smiles_vocab_size=smiles_vocab_size,\n",
    "    rna_vocab_size=rna_vocab_size,\n",
    "    d_model=256,          # 强制嵌入层输出维度=256\n",
    "    nhead=8,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=3\n",
    ").to(device)\n",
    "\n",
    "# --------------------------\n",
    "# 3. 修复权重加载：确保嵌入层权重被正确加载\n",
    "# --------------------------\n",
    "# 加载原始权重\n",
    "state_dict = torch.load(\"/root/autodl-tmp/model_epoch_59.pth\", map_location=device)\n",
    "\n",
    "# 查看所有权重键，确认嵌入层键是否存在（如 \"smiles_embedding.weight\"）\n",
    "print(\"原始权重中的嵌入层相关键：\")\n",
    "for key in state_dict.keys():\n",
    "    if \"embedding\" in key:\n",
    "        print(f\"- {key}: 形状 {state_dict[key].shape}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 过滤权重：保留所有与当前模型匹配的键（包括嵌入层、位置编码、输出层）\n",
    "filtered_state_dict = {}\n",
    "model_keys = set(model.state_dict().keys())  # 当前模型需要的键\n",
    "for key, value in state_dict.items():\n",
    "    if key in model_keys:\n",
    "        # 额外验证嵌入层权重维度是否正确（如 smiles_embedding.weight 形状应为 (vocab_size, 256)）\n",
    "        if \"embedding.weight\" in key:\n",
    "            assert value.shape[-1] == model.d_model, \\\n",
    "                f\"{key} 维度错误：期望 {model.d_model}，实际 {value.shape[-1]}\"\n",
    "        filtered_state_dict[key] = value\n",
    "    else:\n",
    "        print(f\"跳过冗余键：{key}\")\n",
    "\n",
    "# 加载过滤后的权重\n",
    "model.load_state_dict(filtered_state_dict, strict=False)  # strict=False：允许模型有未加载的键（如无）\n",
    "model.eval()\n",
    "print(\"权重加载成功！\")\n",
    "\n",
    "# --------------------------\n",
    "# 4. 修复生成函数：确保SMILES编码后维度正确\n",
    "# --------------------------\n",
    "def generate_aptamers(\n",
    "    model, smiles, smiles_tokenizer, rna_tokenizer,\n",
    "    max_len=80, num_return=5,\n",
    "    strategy=\"topk\", top_k=10, top_p=0.9, temperature=0.8,\n",
    "    device=None\n",
    "):\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # 验证特殊token\n",
    "    required_tokens = [\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"]\n",
    "    for token in required_tokens:\n",
    "        token_id = rna_tokenizer.token_to_id(token)\n",
    "        if token_id is None:\n",
    "            raise ValueError(f\"RNA tokenizer missing required token: {token}\")\n",
    "    \n",
    "    bos_id = rna_tokenizer.token_to_id(\"<bos>\")\n",
    "    eos_id = rna_tokenizer.token_to_id(\"<eos>\")\n",
    "    pad_id = rna_tokenizer.token_to_id(\"<pad>\")\n",
    "    unk_id = rna_tokenizer.token_to_id(\"<unk>\")\n",
    "    smiles_pad_id = smiles_tokenizer.token_to_id(\"<pad>\")\n",
    "    if smiles_pad_id is None:\n",
    "        raise ValueError(\"SMILES tokenizer missing <pad> token\")\n",
    "    \n",
    "    # --------------------------\n",
    "    # 修复SMILES编码：确保输入到嵌入层的张量格式正确\n",
    "    # --------------------------\n",
    "    try:\n",
    "        smi_encoded = smiles_tokenizer.encode(smiles)\n",
    "        max_smi_len = 128  # 与训练时一致\n",
    "        # 补全/截断SMILES到max_smi_len\n",
    "        smi_ids = smi_encoded.ids[:max_smi_len]\n",
    "        smi_ids += [smiles_pad_id] * (max_smi_len - len(smi_ids))\n",
    "        # 转换为张量：(batch_size=1, seq_len=128)\n",
    "        smi_ids = torch.tensor(smi_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        assert smi_ids.shape == (1, max_smi_len), \\\n",
    "            f\"SMILES张量形状错误：期望 (1, {max_smi_len})，实际 {smi_ids.shape}\"\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to encode SMILES: {str(e)}\")\n",
    "    \n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        # --------------------------\n",
    "        # 验证Encoder输入维度（嵌入层输出应为256）\n",
    "        # --------------------------\n",
    "        smiles_emb = model.smiles_embedding(smi_ids)\n",
    "        assert smiles_emb.shape == (1, max_smi_len, model.d_model), \\\n",
    "            f\"SMILES嵌入后形状错误：期望 (1, {max_smi_len}, {model.d_model})，实际 {smiles_emb.shape}\"\n",
    "        \n",
    "        # 预计算Encoder输出\n",
    "        memory = model.encoder(smiles_emb)\n",
    "        \n",
    "        for _ in range(num_return):\n",
    "            generated = [bos_id]\n",
    "            has_unk = False\n",
    "            \n",
    "            for _ in range(max_len - 1):\n",
    "                # RNA输入张量：(1, current_len)\n",
    "                rna_inp = torch.tensor([generated], dtype=torch.long).to(device)\n",
    "                \n",
    "                # 模型前向传播\n",
    "                logits = model(smi_ids, rna_inp)[:, -1, :]  # (1, rna_vocab_size)\n",
    "                \n",
    "                # 温度调节\n",
    "                logits = logits / temperature\n",
    "                \n",
    "                # 采样策略\n",
    "                if strategy == \"greedy\":\n",
    "                    next_id = torch.argmax(logits, dim=-1).item()\n",
    "                elif strategy == \"topk\":\n",
    "                    topk_probs, topk_ids = torch.topk(logits, k=top_k)\n",
    "                    topk_probs = F.softmax(topk_probs, dim=-1)\n",
    "                    idx = torch.multinomial(topk_probs, 1).item()\n",
    "                    next_id = topk_ids[0, idx].item()\n",
    "                elif strategy == \"topp\":\n",
    "                    sorted_logits, sorted_ids = torch.sort(logits, descending=True)\n",
    "                    sorted_probs = F.softmax(sorted_logits, dim=-1)\n",
    "                    cum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "                    cutoff = max(1, torch.sum(cum_probs <= top_p).item())\n",
    "                    filtered_probs = sorted_probs[:, :cutoff]\n",
    "                    filtered_ids = sorted_ids[:, :cutoff]\n",
    "                    idx = torch.multinomial(filtered_probs, 1).item()\n",
    "                    next_id = filtered_ids[0, idx].item()\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "                \n",
    "                # 终止条件\n",
    "                if next_id == unk_id:\n",
    "                    has_unk = True\n",
    "                if next_id == eos_id:\n",
    "                    break\n",
    "                generated.append(next_id)\n",
    "            \n",
    "            # 后处理\n",
    "            filtered_ids = [id for id in generated if id not in [bos_id, eos_id, pad_id, unk_id]]\n",
    "            try:\n",
    "                seq = rna_tokenizer.decode(filtered_ids, skip_special_tokens=True)\n",
    "                if seq and not has_unk and seq not in results:\n",
    "                    results.append(seq)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to decode sequence: {str(e)}\")\n",
    "    \n",
    "    # 补全候选数量\n",
    "    while len(results) < num_return:\n",
    "        results.append(results[-1] if results else \"\")\n",
    "    return results[:num_return]\n",
    "\n",
    "# --------------------------\n",
    "# 5. 测试生成（成功运行）\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 输入SMILES（苯酚）\n",
    "    smiles = \"C1=CC=C(C=C1)O\"\n",
    "    \n",
    "    # 生成前先验证模型嵌入层维度\n",
    "    print(f\"模型嵌入层输出维度：{model.d_model}\")\n",
    "    print(f\"SMILES嵌入层权重形状：{model.smiles_embedding.weight.shape}\")\n",
    "    print(f\"RNA嵌入层权重形状：{model.rna_embedding.weight.shape}\")\n",
    "    \n",
    "    # 生成Aptamer\n",
    "    candidates = generate_aptamers(\n",
    "        model=model,\n",
    "        smiles=smiles,\n",
    "        smiles_tokenizer=smiles_tokenizer,\n",
    "        rna_tokenizer=rna_tokenizer,\n",
    "        max_len=80,\n",
    "        num_return=5,\n",
    "        strategy=\"topk\",\n",
    "        top_k=10,\n",
    "        temperature=0.8,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # 打印结果\n",
    "    print(\"\\n候选Aptamer序列：\")\n",
    "    for i, seq in enumerate(candidates, 1):\n",
    "        print(f\"{i}. {seq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "05cc01f5-dfa5-40cf-b720-6b751c27994d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成的Aptamer序列：\n",
      "候选1: Ġ GGU CUU AC GU C GUU GAU GG GGGU AGC AA G\n",
      "候选2: Ġ CA CGG GAU UUU GC GGU AGG CA GUU CC GUCC UU CGG CC GU CGU GGGG GGU AUCC CU CC GCC UU CC GUCC AACC\n",
      "候选3: Ġ AA GCU UU UUU GACU AUU A GAA GAU CGG CC AU GAA AGCC CC AC AU GCGU GCU UU GCC C GUU ACC AC GU GC AC GC\n"
     ]
    }
   ],
   "source": [
    "# 自定义生成参数（适合调整多样性、长度等）\n",
    "aptamers = generate_aptamers(\n",
    "    model=model,\n",
    "    smiles=\"CC(=O)OC1=CC=CC=C1C(=O)O\",  \n",
    "    smiles_tokenizer=smiles_tokenizer,\n",
    "    rna_tokenizer=rna_tokenizer,\n",
    "    max_len=100,               # 最长生成100个token\n",
    "    num_return=3,              # 返回3个候选序列\n",
    "    strategy=\"topp\",           # 使用top-p采样（核采样）\n",
    "    top_p=0.95,                # 累积概率阈值0.95\n",
    "    temperature=0.8,           # 降低多样性（值越小越确定）\n",
    ")\n",
    "\n",
    "print(\"生成的Aptamer序列：\")\n",
    "for i, seq in enumerate(aptamers):\n",
    "    print(f\"候选{i+1}: {seq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d12537b2-8140-404f-ad58-dc6b48bebbf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting RNA\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/36/1d/8ac00264df042f96c5e5efb1ac03000024a355d490f012fbe59b15da26d4/rna-0.13.2-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /root/miniconda3/lib/python3.12/site-packages (from RNA) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.22 in /root/miniconda3/lib/python3.12/site-packages (from RNA) (1.26.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /root/miniconda3/lib/python3.12/site-packages (from matplotlib->RNA) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /root/miniconda3/lib/python3.12/site-packages (from matplotlib->RNA) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /root/miniconda3/lib/python3.12/site-packages (from matplotlib->RNA) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /root/miniconda3/lib/python3.12/site-packages (from matplotlib->RNA) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/lib/python3.12/site-packages (from matplotlib->RNA) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /root/miniconda3/lib/python3.12/site-packages (from matplotlib->RNA) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /root/miniconda3/lib/python3.12/site-packages (from matplotlib->RNA) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /root/miniconda3/lib/python3.12/site-packages (from matplotlib->RNA) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->RNA) (1.16.0)\n",
      "Installing collected packages: RNA\n",
      "Successfully installed RNA-0.13.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install RNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "38bf9e2a-2df4-4223-8615-37eadd65be61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting viennarna\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/03/2f/73ea7b15dc226f120dbc89f3f4bb40b6a5a26ac969898255f22123a94d1a/ViennaRNA-2.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: viennarna\n",
      "Successfully installed viennarna-2.7.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install viennarna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "983069b2-4860-4215-a600-0b23e558c29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#过滤函数（RNAfold计算ΔG）\n",
    "import RNA\n",
    "\n",
    "def filter_by_rnafold(sequences, min_length=20, max_length=80, max_homopolymer=6, max_candidates=5):\n",
    "    \"\"\"\n",
    "    sequences: list of str\n",
    "    返回过滤+排序后的序列\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for seq in sequences:\n",
    "        # 长度限制\n",
    "        if len(seq) < min_length or len(seq) > max_length:\n",
    "            continue\n",
    "        # 去掉长同聚核苷酸 (AAAAAAA)\n",
    "        if any(base*max_homopolymer in seq for base in \"ACGU\"):\n",
    "            continue\n",
    "        # 用 RNAfold 预测 ΔG\n",
    "        structure, mfe = RNA.fold(seq)\n",
    "        results.append((seq, mfe))\n",
    "\n",
    "    # 按 ΔG 从低到高排序（越低越稳定）\n",
    "    results = sorted(results, key=lambda x: x[1])\n",
    "    return results[:max_candidates]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dd9811cc-fdc4-4b05-88df-6e214c6e560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_filter(\n",
    "    model, smiles, smiles_tokenizer, rna_tokenizer,\n",
    "    num_generate=50, return_top=5,\n",
    "    strategy=\"topk\", top_k=10, top_p=0.9, temperature=0.8\n",
    "):\n",
    "    # Step1: 生成候选\n",
    "    candidates = generate_aptamers(\n",
    "        model, smiles, smiles_tokenizer, rna_tokenizer,\n",
    "        max_len=80, num_return=num_generate,\n",
    "        strategy=strategy, top_k=top_k, top_p=top_p, temperature=temperature,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Step2: 过滤 & 打分\n",
    "    filtered = filter_by_rnafold(candidates, max_candidates=return_top)\n",
    "\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d02cef9a-fbde-41fe-9734-e56e9d3a9ad2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 适配体候选（按ΔG排序）：\n",
      "Ġ CGA GAGG AGU GGU GG GGU CA GAU GCA CU CGG ACC CC AUU CU CC C   ΔG=-4.10\n",
      "Ġ CA AUGG CC ACC CC GG GGU GG GCGC GAA AGU GGU   ΔG=-3.10\n",
      "Ġ CU CU CGG GA CGA CC CA CGU CC GGGU GG CUU GAU AGG GG GGU GGU CC AUCC CU CC   ΔG=-2.20\n",
      "Ġ CC GGU ACA CA GG AGG CU GGU GCGC GGU GAA GU GCC GA GU CGU AA C   ΔG=-2.10\n",
      "Ġ CGU AC GACU CA GG GCC A GAGG GAU CGG GU GGU CGU GGU CCU GAU GCA AUCU CU CC C   ΔG=-1.80\n"
     ]
    }
   ],
   "source": [
    "smiles = \"C1=CC=C(C=C1)O\"  # phenol\n",
    "\n",
    "top_candidates = generate_and_filter(\n",
    "    model, smiles, smiles_tokenizer, rna_tokenizer,\n",
    "    num_generate=100, return_top=5,\n",
    "    strategy=\"topp\", top_p=0.9, temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Top 适配体候选（按ΔG排序）：\")\n",
    "for seq, mfe in top_candidates:\n",
    "    print(f\"{seq}   ΔG={mfe:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc0f72d-3c33-45df-810d-26762e59a637",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_mol2aptamer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
